
#run cutadapt, detect and trim my custom landing pads
cutadapt -a TTGGTGCTGATATTGC...GAAGATAGAGCGACAG -a CTGTCGCTCTATCTTC...GCAATATCAGCACCAA -o subset_landing_pad.fastq subset.fastq --discard-untrimmed

#with checking reverse complement reads insteadd of putting -a twice
#I checked and they find the same number of instances of th adapters
#also changed file names
  #THIS ONE#########################33
cutadapt -a TTGGTGCTGATATTGC...GAAGATAGAGCGACAG -O 10 --discard-untrimmed --revcomp -o all_reads_custom_landing_pads_trimmed.fastq FBC73506_fastq_pass_d5fa85e0_b727e37a_0.fastq > all_reads_custom_landing_pads_trimmed.txt

### OUTPUT: 
#   Done           00:13:51    20,226,344 reads @  41.1 Âµs/read;   1.46 M reads/minute

  
#check if adapter is founf midstrand: 
cutadapt -a TTGGTGCTGATATTGC --rest-file rest_chimeras.fastq --discard-untrimmed --revcomp -o subset_landing_pad.fastq subset.fastq > rest_revcomp_subset_landing_pad.txt
head bi

#cutadapt cross
#cutadapt -g file:cross_barcodes.fasta --revcomp --rest-file cross_chimeras.fastq -o crossALL_landing-pad-rc_subset.fastq --discard-untrimmed landing-pad-rc_subset.fastq > crossALL_landing-pad-rc_subset.txt

#okay now get sep files: 

segment_array=("S" "M" "L")

#search for the cross, plaque, and segment info with cutadapt in loops. I think its worth noting that the loop searches with replacement. ie, reads can be classified multiple times

  #THIS ONE
  for cross_barcode_seq in $(grep -v "^>" "cross_barcodes.fasta" ); do
      
    #get line in barcodes fasta with cross sequence
    cross_seq_line=$(sed -n "/${cross_barcode_seq}/=" cross_barcodes.fasta);        
    #get line in barcodes fasta with fasta name
    cross_header_line=$((cross_seq_line -1)); 
    cross_header=$(sed -n "${cross_header_line}p" cross_barcodes.fasta);
    cross_header=${cross_header#*>};
    echo "cross = ${cross_header}";  
    #O=14; tot len =18 (75%)
    cutadapt -a ${cross_barcode_seq} -O 14 --revcomp --discard-untrimmed --rest-file ${cross_header}_chimeras.fastq -o ${cross_header}_reads.fastq all_reads_custom_landing_pads_trimmed.fastq > ${cross_header}_reads.txt;

      for plaque_barcode_seq in $(grep -v "^>" "plaque_barcodes.fasta" ); do
        
      #get line in barcodes fasta with plaque sequence
      plaque_seq_line=$(sed -n "/${plaque_barcode_seq}/=" plaque_barcodes.fasta);        
      #get line in barcodes fasta with fasta name
      plaque_header_line=$((plaque_seq_line -1)); 
      plaque_header=$(sed -n "${plaque_header_line}p" plaque_barcodes.fasta);
      plaque_header=${plaque_header#*>};
        
      plaque_num="${plaque_header#*plaque}";
      cross_num="${cross_header#*cross}";
      echo "plaque = ${plaque_header}";  

      mkdir barcode${cross_num}${plaque_num};

      #O=18, tot length = 24 (75%)
      cutadapt -g ${plaque_barcode_seq} -O 14 --no-indels --revcomp --discard-untrimmed -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq  ${cross_header}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.txt;

    

     # for segment in "${segment_array[@]}"; do

        #S O= 15, (75%)
        cutadapt -a CTTTCGTACAACCGAGTAGG...CTCCTGAAGTATCTCACGCC -O 15 --revcomp --discard-untrimmed -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads.fastq barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads.txt;

        #M O= 13, (75% of the sorter one)
        cutadapt -a CGCTACGGCGGTATTGTC...GCTCACCAAGTAAGGTGTAGTAT -O 13 --revcomp --discard-untrimmed -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads.fastq barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads.txt;

        #L O=12 (75% of the sorter one)
        cutadapt -a TCGATGTTCAACTACTACGC...GCGAGACTCGCTTTGC -O 12 --revcomp --discard-untrimmed -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads.fastq barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads.txt;    
    done
done




#plaques: 
 for plaque_barcode_seq in $(grep -v "^>" "plaque_barcodes.fasta" ); do
      
    #get line in barcodes fasta with plaque sequence
    plaque_seq_line=$(sed -n "/${plaque_barcode_seq}/=" plaque_barcodes.fasta);        
    #get line in barcodes fasta with fasta name
    plaque_header_line=$((plaque_seq_line -1)); 
    plaque_header=$(sed -n "${plaque_header_line}p" plaque_barcodes.fasta);
    plaque_header=${plaque_header#*>};
    echo "plaque = ${plaque_header}";    
    cutadapt -g ${plaque_barcode_seq} -O 14 --no-indels --revcomp --rest-file ${plaque_header}_chimeras.fastq -o ${plaque_header}_landing-pad-rc_subset.fastq --discard-untrimmed big_landing-pad-rc_subset.fastq > ${plaque_header}_landing-pad-rc_subset.txt;
  done
      


c


################## ADD LENGTH FILTERING #######################################3333
 #THIS ONE
  for cross_barcode_seq in $(grep -v "^>" "cross_barcodes.fasta" ); do
      
    #get line in barcodes fasta with cross sequence
    cross_seq_line=$(sed -n "/${cross_barcode_seq}/=" cross_barcodes.fasta);        
    #get line in barcodes fasta with fasta name
    cross_header_line=$((cross_seq_line -1)); 
    cross_header=$(sed -n "${cross_header_line}p" cross_barcodes.fasta);
    cross_header=${cross_header#*>};
    echo "cross = ${cross_header}";  
    #O=14; tot len =18 (75%)
    #cutadapt -a ${cross_barcode_seq} -O 14 --revcomp --discard-untrimmed --rest-file ${cross_header}_chimeras.fastq -o ${cross_header}_reads.fastq all_reads_custom_landing_pads_trimmed.fastq > ${cross_header}_reads.txt;

      for plaque_barcode_seq in $(grep -v "^>" "plaque_barcodes.fasta" ); do
        
      #get line in barcodes fasta with plaque sequence
      plaque_seq_line=$(sed -n "/${plaque_barcode_seq}/=" plaque_barcodes.fasta);        
      #get line in barcodes fasta with fasta name
      plaque_header_line=$((plaque_seq_line -1)); 
      plaque_header=$(sed -n "${plaque_header_line}p" plaque_barcodes.fasta);
      plaque_header=${plaque_header#*>};
        
      plaque_num="${plaque_header#*plaque}";
      cross_num="${cross_header#*cross}";
      echo "plaque = ${plaque_header}";  

      #mkdir barcode${cross_num}${plaque_num};

      #O=18, tot length = 24 (75%)
      #cutadapt -g ${plaque_barcode_seq} -O 14 --no-indels --revcomp --discard-untrimmed -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq  ${cross_header}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.txt;

    

     # for segment in "${segment_array[@]}"; do

        #S O= 15, (75%)
        # S amplicon length = 621, filter reads shorter than 120 bp and longer than 850 bp
        cutadapt -a CTTTCGTACAACCGAGTAGG...CTCCTGAAGTATCTCACGCC -O 15 --revcomp --discard-untrimmed -m 120 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads_filt_tooshort.fastq -M 850 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads_filt_toolong.fastq -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads_filt.fastq barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads_filt.txt;

        #M O= 13, (75% of the sorter one)
        # M amplicon length = 443, filter reads shorter than 100 bp and longer than 670 bp
        cutadapt -a CGCTACGGCGGTATTGTC...GCTCACCAAGTAAGGTGTAGTAT -O 13 --revcomp --discard-untrimmed -m 100 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads_filt_tooshort.fastq -M 670 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads_filt_toolong.fastq -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads_filt.fastq barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads_filt.txt;

        #L O=12 (75% of the sorter one)
        # L amplicon length = 941, filter reads shorter than 200 bp and longer than 1200 bp
        cutadapt -a TCGATGTTCAACTACTACGC...GCGAGACTCGCTTTGC -O 12 --revcomp --discard-untrimmed -m 200 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads_filt_tooshort.fastq -M 1200 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads_filt_toolong.fastq  -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads_filt.fastq barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads_filt.txt;    
    done
done















#cuutadapt plaque
  cutadapt -a file:plaque_barcodes.fasta --revcomp --rest-file plaquel_chimeras.fastq -o cross_landing-pad-rc_subset.fastq --discard-untrimmed landing-pad-rc_subset.fastq > cross_landing-pad-rc_subset.txt




#once filtered, cutadapt and




#actual line to run genotyping and usearch
bash genotyping_new.sh -d /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt -e /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/sample_list2.csv -c /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/cross_list.txt -s /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/refs/ref_phi6_S_04.fasta -m /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/refs/ref_phi6_M_01.fasta -l /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/refs/ref_phi6_L_89.fasta |& tee genotyping_usearch_rerun.out


0.15 ERROR!!!!! #actual line to run genotyping and usearch
bash genotyping_new.sh -d /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15 -e /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/sample_list2.csv -c /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/cross_list.txt -s /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/refs/ref_phi6_S_04.fasta -m /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/refs/ref_phi6_M_01.fasta -l /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/refs/ref_phi6_L_89.fasta |& tee genotyping_usearch_error0.15.out




for bsix in 









srun -c 2 -t 20:00:00 --mem=30000 --partition=low --pty /bin/bash
module load conda
conda activate cutadapt




#higher error

  #THIS ONE#########################33
cutadapt -a TTGGTGCTGATATTGC...GAAGATAGAGCGACAG -e 0.15 -O 10 --discard-untrimmed --revcomp -o all_reads_custom_landing_pads_trimmed_error0.15.fastq FBC73506_fastq_pass_d5fa85e0_b727e37a_0.fastq > all_reads_custom_landing_pads_trimmed_error0.15.txt



#THIS ONE
  for cross_barcode_seq in $(grep -v "^>" "cross_barcodes.fasta" ); do
      
    #get line in barcodes fasta with cross sequence
    cross_seq_line=$(sed -n "/${cross_barcode_seq}/=" cross_barcodes.fasta);        
    #get line in barcodes fasta with fasta name
    cross_header_line=$((cross_seq_line -1)); 
    cross_header=$(sed -n "${cross_header_line}p" cross_barcodes.fasta);
    cross_header=${cross_header#*>};
    echo "cross = ${cross_header}";  
    #O=14; tot len =18 (75%)
    cutadapt -a ${cross_barcode_seq} -O 14 --no-indels -e 0.15 --revcomp --discard-untrimmed --rest-file ${cross_header}_chimeras.fastq -o ${cross_header}_reads.fastq all_reads_custom_landing_pads_trimmed_error0.15.fastq > ${cross_header}_reads.txt;

      for plaque_barcode_seq in $(grep -v "^>" "plaque_barcodes.fasta" ); do
        
      #get line in barcodes fasta with plaque sequence
      plaque_seq_line=$(sed -n "/${plaque_barcode_seq}/=" plaque_barcodes.fasta);        
      #get line in barcodes fasta with fasta name
      plaque_header_line=$((plaque_seq_line -1)); 
      plaque_header=$(sed -n "${plaque_header_line}p" plaque_barcodes.fasta);
      plaque_header=${plaque_header#*>};
        
      plaque_num="${plaque_header#*plaque}";
      cross_num="${cross_header#*cross}";
      echo "plaque = ${plaque_header}";  

      mkdir barcode${cross_num}${plaque_num};

      #O=18, tot length = 24 (75%)
      cutadapt -g ${plaque_barcode_seq} -O 14 -e 0.15 --no-indels --revcomp --discard-untrimmed -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq  ${cross_header}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.txt;

    

     # for segment in "${segment_array[@]}"; do

        #S O= 15, (75%)
        # S amplicon length = 621, filter reads shorter than 120 bp and longer than 850 bp
        cutadapt -a CTTTCGTACAACCGAGTAGG...CTCCTGAAGTATCTCACGCC -e 0.15 -O 15 --no-indels --revcomp --discard-untrimmed -m 120 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads_filt_tooshort.fastq -M 850 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads_filt_toolong.fastq -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads_filt.fastq barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_S_reads_filt.txt;

        #M O= 13, (75% of the sorter one)
        # M amplicon length = 443, filter reads shorter than 100 bp and longer than 670 bp
        cutadapt -a CGCTACGGCGGTATTGTC...GCTCACCAAGTAAGGTGTAGTAT -e 0.15 -O 13 --no-indels --revcomp --discard-untrimmed -m 100 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads_filt_tooshort.fastq -M 670 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads_filt_toolong.fastq -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads_filt.fastq barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_M_reads_filt.txt;

        #L O=12 (75% of the sorter one)
        # L amplicon length = 941, filter reads shorter than 200 bp and longer than 1200 bp
        cutadapt -a TCGATGTTCAACTACTACGC...GCGAGACTCGCTTTGC -e 0.15 -O 12 --no-indels --revcomp --discard-untrimmed -m 200 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads_filt_tooshort.fastq -M 1200 --too-short-output barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads_filt_toolong.fastq  -o barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads_filt.fastq barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_reads.fastq > barcode${cross_num}${plaque_num}/barcode${cross_num}${plaque_num}_L_reads_filt.txt;    
    done
done






Go back and usearch parents for new data:

bash genotyping_with_parents.sh -d /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15 -e /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/sample_list2_parents.csv -c /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/cross_list_p.txt -s /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/refs/ref_phi6_S_04.fasta -m /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/refs/ref_phi6_M_01.fasta -l /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/error0.15/refs/ref_phi6_L_89.fasta |& tee genotyping_usearch_error0.15.out


old data: 

bash genotyping_with_parents.sh -d /group/sldmunozgrp/cysto_coinf_CA68x426427 -e /group/sldmunozgrp/cysto_coinf_CA68x426427/sample_list2_parents.csv -c /group/sldmunozgrp/cysto_coinf_CA68x426427/cross_list_p.txt -s /group/sldmunozgrp/cysto_coinf_CA68x426427/refs/ref_phi6_S_48.fasta -m /group/sldmunozgrp/cysto_coinf_CA68x426427/refs/ref_phi6_M_45.fasta -l /group/sldmunozgrp/cysto_coinf_CA68x426427/refs/ref_phi6_L_89.fasta |& tee genotyping_usearch_with parents.out







#starting over, 9/10/2025


#lets actually look for the ONT adaptyers weith cutadapt


cutadapt -a TTTTTTTTCCTGTACTTCGTTCAGTTACGTATTGCT -a GCAATACGTAACTGAACGAAGTACAGG --revcomp -o subset_adapter_tests.fastq big_subset.fastq --action=lowercase --untrimmed-output=untrimmed.fastq > running_log.txt
####
Total reads processed:                 100,000
Reads with adapters:                    69,499 (69.5%)
Reverse-complemented:                   45,778 (45.8%)

== Read fate breakdown ==
Reads discarded as untrimmed:           30,501 (30.5%)
Reads written (passing filters):        69,499 (69.5%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     62,753,566 bp (71.1%)
#####
#ok the adapters seem to be at the ends...? I'm confused, -a is supposed to be for 5' adapters
# later update: oh ok i see now there is a typo on the cutadapt site >:( - -g is actually for 5'


#switch to -g, just to see
cutadapt -g TTTTTTTTCCTGTACTTCGTTCAGTTACGTATTGCT -g GCAATACGTAACTGAACGAAGTACAGG --revcomp -o subset_adapter_tests.fastq big_subset.fastq --action=lowercase --untrimmed-output=untrimmed.fastq > running_log.txt

####
Total reads processed:                 100,000
Reads with adapters:                    66,690 (66.7%)
Reverse-complemented:                   44,606 (44.6%)

== Read fate breakdown ==
Reads discarded as untrimmed:           33,310 (33.3%)
Reads written (passing favgilters):        66,690 (66.7%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     61,577,490 bp (69.8%)
#####
#weirdly similar? lots of the reads appear to be ALL adapter... 
#lets do both:


cutadapt -g TTTTTTTTCCTGTACTTCGTTCAGTTACGTATTGCT -g GCAATACGTAACTGAACGAAGTACAGG -a TTTTTTTTCCTGTACTTCGTTCAGTTACGTATTGCT -a GCAATACGTAACTGAACGAAGTACAGG --revcomp -o subset_adapter_tests.fastq big_subset.fastq --action=lowercase --untrimmed-output=untrimmed.fastq > running_log.txt

#meh ok thats barely higher:

######
Total reads processed:                 100,000
Reads with adapters:                    71,879 (71.9%)
Reverse-complemented:                   45,027 (45.0%)

== Read fate breakdown ==
Reads discarded as untrimmed:           28,121 (28.1%)
Reads written (passing filters):        71,879 (71.9%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     64,772,461 bp (73.4%)
######

## e=0.15:

cutadapt -g TTTTTTTTCCTGTACTTCGTTCAGTTACGTATTGCT -g GCAATACGTAACTGAACGAAGTACAGG -a TTTTTTTTCCTGTACTTCGTTCAGTTACGTATTGCT -a GCAATACGTAACTGAACGAAGTACAGG --revcomp -e 0.15 -o subset_both_adapter_tests.fastq big_subset.fastq --action=lowercase --untrimmed-output=ont-adapters_untrimmed.fastq > running_log.txt

#### significant increase in reads haivng the adapter?
####
Total reads processed:                 100,000
Reads with adapters:                    88,487 (88.5%)
Reverse-complemented:                   44,912 (44.9%)

== Read fate breakdown ==
Reads discarded as untrimmed:           11,513 (11.5%)
Reads written (passing filters):        88,487 (88.5%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     79,248,723 bp (89.8%)
#####

#### realized i can do the same thing (search both ends) with -b instead




#ok lets just try thr landing pads again 

cutadapt -a TTGGTGCTGATATTGC -g GAAGATAGAGCGACAG -O 6 --revcomp -o subset_landpad_tests.fastq big_subset.fastq --action=lowercase --untrimmed-output=untrimmed.fastq > running_log.txt

##############
Total reads processed:                 100,000
Reads with adapters:                    81,833 (81.8%)
Reverse-complemented:                   46,709 (46.7%)

== Read fate breakdown ==
Reads discarded as untrimmed:           18,167 (18.2%)
Reads written (passing filters):        81,833 (81.8%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     77,671,585 bp (88.0%)
#######################


#what if i search on the output of the adapter search?

cutadapt -a TTGGTGCTGATATTGC -g GAAGATAGAGCGACAG -O 6 --revcomp -o subset_landpad_tests.fastq subset_both_adapter_tests.fastq --action=lowercase --untrimmed-output=untrimmed.fastq > running_log.txt

#not as high a % as I would have guessed 

Total reads processed:                  88,487
Reads with adapters:                    74,711 (84.4%)
Reverse-complemented:                   31,260 (35.3%)

== Read fate breakdown ==
Reads discarded as untrimmed:           13,776 (15.6%)
Reads written (passing filters):        74,711 (84.4%)

Total basepairs processed:    79,248,723 bp
Total written (filtered):     70,845,786 bp (89.4%)

#okay maybe actually need to search on  TRIMMED adapter search results (take off previous actio =lowercase) 












#lets try looking for mid-strand adapters with porechop

porechop -i cutadapt/big_subset.fastq --verbosity 2 --extra_end_trim 0 --end_size 40 --min_split_read_size 1 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o porechop/split_subset_reads.fastq 


grep "adapters" recompile_rerun_new_adapters.txt

#Trimming adapters from read ends
# 83,335 / 100,000 reads had adapters trimmed from their start (2,760,017 bp removed)
# 26,147 / 100,000 reads had adapters trimmed from their end (256,528 bp removed)
# Splitting reads containing middle adapters
# 3,320 / 100,000 reads were split based on middle adapters


#that doesnt add up to 100,000, its actually around 109K, so some reads have 2 adapters. not sure what % of reads have none.
#also looks like only around 3% had middle adapters
#visually, it looks like ~17/170 or 10% dont have an adapter identified at either end

#rerun with higher error allowed

porechop -i cutadapt/big_subset.fastq --verbosity 2 --end_threshold 60 --middle_threshold 80 --extra_end_trim 0 --end_size 40 --min_split_read_size 1 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o porechop/split_adapters-only_subset_reads.fastq  > 


Looking for known adapter sets
Trimming adapters from read ends
 92,331 / 100,000 reads had adapters trimmed from their start (3,038,744 bp removed)
 37,368 / 100,000 reads had adapters trimmed from their end (503,233 bp removed)
Splitting reads containing middle adapters
6,288 / 100,000 reads were split based on middle adapters


#It looks like the adapters are present in those even where they arent identified, but at very high error. Lets try to do some QC analysis.

#nanoQC
nanoQC -o nanoQC cutadapt/big_subset.fastq
#plots show that quality doesnt level off until around base 45 or so, and then at the ends it drops starting at around base -30 or so. So it makes sense that some of the end adapters have high error 


#hmm... what if we just look for the pcr 2 landing pads

cutadapt -a TTGGTGCTGATATTGC...GAAGATAGAGCGACAG -O 10 --discard-untrimmed --revcomp -o all_reads_pcr2_landing_pads_trimmed.fastq big_subset.fastq > current_cutadapt_log.txt
Total reads processed:                 100,000
Reads with adapters:                    81,820 (81.8%)
Reverse-complemented:                   52,223 (52.2%)

== Read fate breakdown ==
Reads discarded as untrimmed:           18,180 (18.2%)
Reads written (passing filters):        81,820 (81.8%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     45,884,667 bp (52.0%)


cutadapt -g TTGGTGCTGATATTGC -a GAAGATAGAGCGACAG -O 8 --revcomp --action=lowercase --untrimmed-output=untrimmed_landing_pads.fastq -o all_reads_pcr2_landing_pads_trimmed.fastq big_subset.fastq > current_cutadapt_log.txt


Total reads processed:                 100,000
Reads with adapters:                    81,830 (81.8%)
Reverse-complemented:                   46,724 (46.7%)

== Read fate breakdown ==
Reads discarded as untrimmed:           18,170 (18.2%)
Reads written (passing filters):        81,830 (81.8%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     77,669,461 bp (88.0%)


#add --times=2
#cutadapt -g TTGGTGCTGATATTGC -a GAAGATAGAGCGACAG -O 8 --revcomp --action=lowercase --untrimmed-output=untrimmed_landing_pads.fastq -o all_reads_pcr2_landing_pads_trimmed.fastq big_subset.fastq -e --times 2 > current_cutadapt_log.txt

Total reads processed:                 100,000
Reads with adapters:                    81,830 (81.8%)
Reverse-complemented:                   51,919 (51.9%)

== Read fate breakdown ==
Reads discarded as untrimmed:           18,170 (18.2%)
Reads written (passing filters):        81,830 (81.8%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     77,669,461 bp (88.0%)

#hmm, doesnt seem to change the overall % of reads with adapters, but it does increase the number of reverse complemented reads that were found.

#increase error to 0.15, allowing 1 error in 10 bases, 2 errors in 20 bases, etc. (previously was 0 in 10, 1 in 20, 2 in 30)
cutadapt -g TTGGTGCTGATATTGC -a GAAGATAGAGCGACAG -O 8 --revcomp --action=lowercase --untrimmed-output=untrimmed_landing_pads.fastq -o all_reads_pcr2_landing_pads_trimmed.fastq big_subset.fastq -e 0.15  --times 2 > current_cutadapt_log.txt

#visual check shows most lowercase bases at the ends of reads? Why isnt it trimming both ends....?

Total reads processed:                 100,000
Reads with adapters:                    84,986 (85.0%)
Reverse-complemented:                   53,779 (53.8%)

== Read fate breakdown ==
Reads discarded as untrimmed:           15,014 (15.0%)
Reads written (passing filters):        84,986 (85.0%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     79,623,082 bp (90.2%)


cutadapt -a TTGGTGCTGATATTGC...GAAGATAGAGCGACAG -O 8 --action=lowercase --untrimmed-output=untrimmed_landing_pads.fastq --revcomp -e 0.15 -o all_reads_pcr2_landing_pads_linked_trimmed.fastq big_subset.fastq > current_cutadapt_log.txt

#linked adapters show exactly the same stats 
#visual check: okay lots still with what appear to be internal adapters. um also, theres a noticeable fraction that appear to only have identified adapters on one end. weird

Total reads processed:                 100,000
Reads with adapters:                    84,986 (85.0%)
Reverse-complemented:                   54,154 (54.2%)

== Read fate breakdown ==
Reads discarded as untrimmed:           15,014 (15.0%)
Reads written (passing filters):        84,986 (85.0%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     79,623,082 bp (90.2%)


#try just one landing pad:
cutadapt -g TTGGTGCTGATATTGC -O 8 --revcomp --action=lowercase --untrimmed-output=untrimmed_landing_pads.fastq -o all_reads_pcr2_landing_pad_5pr_trimmed.fastq big_subset.fastq -e 0.15 --times 2 > current_cutadapt_log.txt

##visual check shows that it sure seems like more than the ~3% identified by porechop that have an internal adapter (ie like way more than just the first <=40 bases are lowercase. in some instances its like half the reads. Turn off --revcomp next and see if this persists.

Total reads processed:                 100,000
Reads with adapters:                    43,958 (44.0%)
Reverse-complemented:                   19,428 (19.4%)

== Read fate breakdown ==
Reads discarded as untrimmed:           56,042 (56.0%)
Reads written (passing filters):        43,958 (44.0%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     54,431,783 bp (61.7%)


cutadapt -g TTGGTGCTGATATTGC -O 8 --action=lowercase --untrimmed-output=untrimmed_landing_pads.fastq -o all_reads_pcr2_landing_pad_5pr_trimmed.fastq big_subset.fastq -e 0.15 --times 2 > current_cutadapt_log.txt

##visual check shows honestly kinda the same pattern? most lowercase bases at the beginning of the reads definitely still a lot that have an internal adapter (ie like way more than just the first <=40 bases are lowercase. in some instances its like half the reads still??  

#loss of 44% -> 27% of reads found with adapters when you take out --revcomp

Total reads processed:                 100,000
Reads with adapters:                    27,590 (27.6%)

== Read fate breakdown ==
Reads discarded as untrimmed:           72,410 (72.4%)
Reads written (passing filters):        27,590 (27.6%)

Total basepairs processed:    88,277,155 bp
Total written (filtered):     37,701,137 bp (42.7%)




#lets try adding the landing pads to porechop & rerunning. 
#deleted the tool, edited adapters.py and recompiled the python scripts
# for now take out --end_threshold 60 --middle_threshold 80  and use defualts
touch /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/ONT-adapters_land-pads_subset_reads.fastq
porechop -i /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/big_subset.fastq --verbosity 2 --extra_end_trim 0 --end_size 40 --min_split_read_size 1 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o group/sldmunozgrp/cysto_LMGSeq08-25/porechop/ONT-adapters_land-pads_subset_reads.fastq  > running_porechop_ONT-adapt_land-pads.txt


#check how many reads have internal adapters when the end size is set to 100 bp. change the min readd size kept post splitting to 50 bp:
porechop -i /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/big_subset.fastq --verbosity 2 --end_threshold 60 --middle_threshold 80  and use defualts --extra_end_trim 0 --end_size 100 --min_split_read_size 50 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/wider-ends_ONT-adapters_land-pads_subset_reads.fastq  > running_porechop_wider-ends_ONT-adapt_land-pads.txt


wc -l file.fastq | awk '{print $1 / 4}'
#num reads: 154841
#get avg lengths: 

#avg read length is 510.433 compared to 882.772 for original subset file
#whoa thats a ot more than the starting 100K


#even wider ends and larger min fragment size
porechop -i /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/big_subset.fastq --verbosity 2 --end_threshold 70 --middle_threshold 80 --extra_end_trim 0 --end_size 150 --min_split_read_size 200 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/even-wider-ends_ONT-adapters_land-pads_subset_reads.fastq  > running_porechop_even-wider-ends_ONT-adapt_land-pads.txt
#num reads: 151839
#avg read length is 539.842 compared to 882.772 for original subset file
#numbers arent that different?

#compare % of reads longer than 1200 bp 
awk 'BEGIN {total=0; longer=0} NR%4==2 {total++; if (length($0)>1051) longer++} END {print longer/total}' ../cutadapt/big_subset.fastq
0.24105

awk 'BEGIN {total=0; longer=0} NR%4==2 {total++; if (length($0)>1051) longer++} END {print longer/total}' even-wider-ends_ONT-adapters_land-pads_subset_reads.fastq
0.0469906

#ok that looks pretty good!
#L amplicon length is 905 (non-primer region) + 60 (fwd primer) +50 (R primer) + 36 for the longer of the two ONT adapters

#trim anything longer than 1051?
cutadapt -M 1051 --untrimmed-output=untrimmed_over_1051.fastq  even-wider-ends_ONT-adapters_land-pads_subset_reads.fastq > length-trimmed_cutadapt_log.txt

Total reads processed:                 151,839

== Read fate breakdown ==
Reads that were too long:                7,135 (4.7%)
Reads discarded as untrimmed:          144,704 (95.3%)
Reads written (passing filters):             0 (0.0%)

Total basepairs processed:    81,969,126 bp
Total written (filtered):              0 bp (0.0%)


#btw, % reads under 200 bp length
awk 'BEGIN {total=0; longer=0} NR%4==2 {total++; if (length($0)<200) longer++} END {print longer/total}' ../cutadapt/big_subset.fastq
#0.04231
awk 'BEGIN {total=0; longer=0} NR%4==2 {total++; if (length($0)<200) longer++} END {print longer/total}' length-trimmed_even-wider-ends_ONT-adapters_land-pads_subset_reads.fastq
#0.0318581










#maybe re-check for adapters? use higher allowed error (lower % id)
porechop -i /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/length-trimmed_even-wider-ends_ONT-adapters_land-pads_subset_reads.fastq --verbosity 2 --end_threshold 60 --middle_threshold 70 --extra_end_trim 0 --end_size 200 --min_split_read_size 200 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/re-check.fastq  > running_porechop_re-check.txt

#nice!
144,704 reads loaded


Looking for known adapter sets
10,000 / 10,000 (100.0%)
                                        Best
                                        read       Best
                                        start      read end
  Set                                   %ID        %ID
  SQK-LSK114                                79.3       74.1
  PCR2_landing_pads                         77.8       78.9


No adapters found - output reads are unchanged from input reads

#try with lower adapter threshold, use 70 due to above numbers

porechop -i /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/length-trimmed_even-wider-ends_ONT-adapters_land-pads_subset_reads.fastq --adapter_threshold 70 --verbosity 2 --end_threshold 60 --middle_threshold 70 --extra_end_trim 0 --end_size 200 --min_split_read_size 200 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/re-check.fastq  > running_porechop_re-check.txt

#ermmm ok when you run with adapt threshold of 70& you gat the following: 
#but when I blast apparent 'hits' they seem to always match fully to cystovirus on blast - i think these arent real because pid is too low

Trimming adapters from read ends
110,880 / 144,704 reads had adapters trimmed from their start (10,241,101 bp removed)
119,473 / 144,704 reads had adapters trimmed from their end (9,141,755 bp removed)
Splitting reads containing middle adapters
71,522 / 144,704 reads were split based on middle adapters






to-do
-split reads on middle adapters
-trim reads of adapters and landing pads, keeping only those where they are found
-demultiplex by barcode F
-demux by barcode R
-demux by primer

#next, look for barcodes?????




#check for adapters in porechoppped data with cutadapt

cutadapt -a TTGGTGCTGATATTGC...GAAGATAGAGCGACAG -O 8 --action=lowercase --untrimmed-output=untrimmed_landing_pads.fastq --revcomp -e 0.15 -o cutadapt-landpads-length-trimmed_even-wider-ends_ONT-adapters_land-pads_subset_reads.fastq length-trimmed_even-wider-ends_ONT-adapters_land-pads_subset_reads.fastq > current_cutadapt_landpads_log.txt

Total reads processed:                 144,704
Reads with adapters:                    72,976 (50.4%)
Reverse-complemented:                   72,547 (50.1%)

== Read fate breakdown ==
Reads discarded as untrimmed:           71,728 (49.6%)
Reads written (passing filters):        72,976 (50.4%)

Total basepairs processed:    70,306,072 bp
Total written (filtered):     36,038,609 bp (51.3%)

###ummmm ok so looks like Porechop isnt doing the reverse complementing?

#rerun but keep the untrimmed
cutadapt -a TTGGTGCTGATATTGC...GAAGATAGAGCGACAG -O 8 --action=lowercase --revcomp -e 0.15 -o cutadapt-landpads-length-trimmed_even-wider-ends_ONT-adapters_land-pads_subset_reads.fastqlength-trimmed_even-wider-ends_ONT-adapters_land-pads_subset_reads.fastq > current_cutadapt_landpads_log.txt


#visually? looks pretty good tbh



### OOOOOOH what if i go back and run porechop with end size ==0??
#so can do read splitting and then just check for adapters with cutadapt??

porechop -i /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/big_subset.fastq --verbosity 2 --end_threshold 100 --middle_threshold 80 --extra_end_trim 0 --end_size 0 --min_split_read_size 200 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/split-only_ONT-adapters_land-pads_subset_reads.fastq  > running_porechop_split-only_ONT-adapt_land-pads.txt





#tried pychopper for detecting midstrand reads

#seems to only thinks that like 25% of reads are usable? weird

#final thoughts for the day - are some nanopore reads reversed? idk

#does porechopper search the reverse orientation? idk

#UMMM REALIZED I HAD THE ADAPTERS SET TO BE FWD/REV AND NOT BOTH FWD in porechop - oops

#fixed and re-ran this
porechop -i /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/big_subset.fastq --verbosity 2 --end_threshold 70 --middle_threshold 80 --extra_end_trim 0 --end_size 150 --min_split_read_size 200 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/ONT-adapters-only_big-subset_reads.fastq  > pclog_ONT-adapters-only.txt

#go back and search for primers in the reads where adapters ot found after trimming low quality areas













# 9-29-2025
###ok, talked to sam, super sus that if you use cutadapt after porechop, 50% of the reads have adapters, and 50% of them were found in th RC... 
#lets go back and try to recompile porechop with RCed adapters

#add RC landpads and adapters, then recompile
python3 setup.py install

#porechop:
porechop -i /group/sldmunozgrp/cysto_LMGSeq08-25/cutadapt/big_subset.fastq --verbosity 2 --end_threshold 70 --middle_threshold 80 --extra_end_trim 0 --end_size 150 --min_split_read_size 200 --extra_middle_trim_good_side 0 --extra_middle_trim_bad_side 0 --min_trim_size 8 -o /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq  > running_porechop_land-pads_with_revcomp.txt


grep "adapters" running_porechop_land-pads_with_revcomp.txt

#Trimming adapters from read ends
# 26,957 / 100,000 reads had adapters trimmed from their start (1,296,843 bp removed)
# 41,122 / 100,000 reads had adapters trimmed from their end (1,862,309 bp removed)
#Splitting reads containing middle adapters
#74,289 / 100,000 reads were split based on middle adapters


#awk to look at splitted read sizes

#compare % of reads longer than 1200 bp 
#whole subset:
awk 'BEGIN {total=0; longer=0} NR%4==2 {total++; if (length($0)>1051) longer++} END {print longer/total}' ../cutadapt/big_subset.fastq
#0.24105

#post trim:
awk 'BEGIN {total=0; longer=0} NR%4==2 {total++; if (length($0)>1051) longer++} END {print longer/total}' /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq
#0.00595968


#% under 200
awk 'BEGIN {total=0; longer=0} NR%4==2 {total++; if (length($0)<200) longer++} END {print longer/total}' /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq
#0.0207205

#just curious - % under 400
awk 'BEGIN {total=0; longer=0} NR%4==2 {total++; if (length($0)<400) longer++} END {print longer/total}' /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq
0.449642

cutadapt -a TTGGTGCTGATATTGC...GAAGATAGAGCGACAG -O 8 --action=lowercase --untrimmed-output=untrimmed_landing_pads.fastq --revcomp -e 0.15 -o cutadapt-trimmed_porechop_land-pads_with_revcomp.fastq /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq > current_cutadapt_landpads_RC_log.txt

#Processing single-end reads on 1 core ...

#=== Summary ===

#Total reads processed:                 177,023
#Reads with adapters:                     1,497 (0.8%)
#Reverse-complemented:                      965 (0.5%)

#== Read fate breakdown ==
#Reads discarded as untrimmed:          175,526 (99.2%)
#Reads written (passing filters):         1,497 (0.8%)

#Total basepairs processed:    79,679,874 bp
#Total written (filtered):        788,324 bp (1.0%)

#nice!!!!

#ok so resulting file has 
#    -no more landing pads, has 
#    -0.6% of reads longer than 1200 bp and 2% of reads under 200 bp - nice
#    -44% under 400 bp




#time to search for barcodes?
#use multiple adapters with cutadapt


#file:cross_barcodes.fasta

cutadapt -a file:cross_barcodes.fasta -O 8 --action=lowercase --revcomp -e 0.15 -o cutadapt-cross-demux_porechop_land-pads_with_revcomp.fastq /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq > current_cutadapt-cross-demux_landpads_RC_log.txt

Total reads processed:                 177,023
Reads with adapters:                   131,820 (74.5%)
Reverse-complemented:                   78,137 (44.1%)
Reads written (passing filters):       177,023 (100.0%)

Total basepairs processed:    79,679,874 bp
Total written (filtered):     79,679,874 bp (100.0%)


#oops, also use {name} in the -o file for demuxing
cutadapt -a file:cross_barcodes.fasta -O 8 --action=lowercase --revcomp -e 0.15 -o cutadapt-cross-{name}-demux_porechop_land-pads_with_revcomp.fastq /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq > current_cutadapt-cross-demux_landpads_RC_log.txt

Total reads processed:                 177,023
Reads with adapters:                   131,820 (74.5%)
Reverse-complemented:                   78,137 (44.1%)
Reads written (passing filters):       177,023 (100.0%)

Total basepairs processed:    79,679,874 bp
Total written (filtered):     79,679,874 bp (100.0%)


mkdir cutadapt_demultiplexing
mv *cutadapt-cross*
cd cutadapt_demultiplexing

for i in *.fastq; do echo "${i}"; wc -l ${i} | awk '{print $1 / 4}'; done

cutadapt-cross-cross01-demux_porechop_land-pads_with_revcomp.fastq 68707
cutadapt-cross-cross02-demux_porechop_land-pads_with_revcomp.fastq 15611
cutadapt-cross-cross03-demux_porechop_land-pads_with_revcomp.fastq 7150
cutadapt-cross-cross04-demux_porechop_land-pads_with_revcomp.fastq 1841
cutadapt-cross-cross05-demux_porechop_land-pads_with_revcomp.fastq 161
cutadapt-cross-cross06-demux_porechop_land-pads_with_revcomp.fastq 2123
cutadapt-cross-cross07-demux_porechop_land-pads_with_revcomp.fastq 1119
cutadapt-cross-cross08-demux_porechop_land-pads_with_revcomp.fastq 16679
cutadapt-cross-cross09-demux_porechop_land-pads_with_revcomp.fastq 55
cutadapt-cross-cross10-demux_porechop_land-pads_with_revcomp.fastq 5216
cutadapt-cross-cross11-demux_porechop_land-pads_with_revcomp.fastq 668
cutadapt-cross-cross12-demux_porechop_land-pads_with_revcomp.fastq 90
cutadapt-cross-cross13-demux_porechop_land-pads_with_revcomp.fastq 24
cutadapt-cross-cross14-demux_porechop_land-pads_with_revcomp.fastq 31
cutadapt-cross-cross15-demux_porechop_land-pads_with_revcomp.fastq 8212
cutadapt-cross-cross16-demux_porechop_land-pads_with_revcomp.fastq 24
cutadapt-cross-cross17-demux_porechop_land-pads_with_revcomp.fastq 45
cutadapt-cross-cross18-demux_porechop_land-pads_with_revcomp.fastq 42
cutadapt-cross-cross19-demux_porechop_land-pads_with_revcomp.fastq 32
cutadapt-cross-cross20-demux_porechop_land-pads_with_revcomp.fastq 3459
cutadapt-cross-cross21-demux_porechop_land-pads_with_revcomp.fastq 92
cutadapt-cross-cross22-demux_porechop_land-pads_with_revcomp.fastq 31
cutadapt-cross-cross23-demux_porechop_land-pads_with_revcomp.fastq 9
cutadapt-cross-cross24-demux_porechop_land-pads_with_revcomp.fastq 10
cutadapt-cross-cross25-demux_porechop_land-pads_with_revcomp.fastq 11
cutadapt-cross-cross26-demux_porechop_land-pads_with_revcomp.fastq 37
cutadapt-cross-cross27-demux_porechop_land-pads_with_revcomp.fastq 168
cutadapt-cross-cross28-demux_porechop_land-pads_with_revcomp.fastq 31
cutadapt-cross-cross29-demux_porechop_land-pads_with_revcomp.fastq 131
cutadapt-cross-cross30-demux_porechop_land-pads_with_revcomp.fastq 11
cutadapt-cross-demux_porechop_land-pads_with_revcomp.fastq 177023
cutadapt-cross-unknown-demux_porechop_land-pads_with_revcomp.fastq 45203



for i in *.fastq; do echo "${i}"; wc -l ${i} | awk '{print $1 / 4}'; awk 'BEGIN {total=0; longer=0} NR%4==2 {total++; if (length($0)<220) longer++} END {print longer/total}' ${i}; done


#raise error to 0.2:

cutadapt -a file:cross_barcodes.fasta -O 8 --action=lowercase --revcomp -e 0.2 -o cutadapt-cross-demux-e.2_porechop_land-pads_with_revcomp.fastq /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq > current_cutadapt-cross-demux_landpads_RC_log.txt

#onlt 3% more reads classified:
Total reads processed:                 177,023
Reads with adapters:                   137,857 (77.9%)
Reverse-complemented:                   82,035 (46.3%)
Reads written (passing filters):       177,023 (100.0%)

Total basepairs processed:    79,679,874 bp
Total written (filtered):     79,679,874 bp (100.0%)



#can we do both with files??

cutadapt -a file:cross_barcodes.fasta -g file:plaque_barcodes.fasta -O 8 --action=lowercase --revcomp -e 0.15 -o cutadapt-{name1}-{name2}-demux_porechop_land-pads_with_revcomp.fastq /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq > current_cutadapt-cross-plaque-demux_landpads_RC_log.txt

#meh didnt work


#okay demultiplex by plaque, input = cross files
for fastq in cutadapt-cross-cross*; 
do cross_temp="${fastq##*cross-}";
cross="${cross_temp%%-demux*}";
cutadapt -g file:plaque_barcodes.fasta -O 8 --action=lowercase --revcomp -e 0.15 -o cutadapt-${cross}-{name}-demux_porechop_land-pads_with_revcomp.fastq ${fastq} > current_cutadapt-${cross}-plaque-demux_landpads_RC_log.txt;
done

#get read counts for each file
for fastq in *plaque*-demux*; do cross_temp="${fastq##*cutadapt-}";cross="${cross_temp%%-demux*}";count=$( wc -l ${fastq} | awk '{print $1 / 4}'); echo "${cross},${count}" >> file_counts.csv; done


#demux on primer, input = cross-plaque files
for fastq in *plaque*-demux*.fastq; 
do crplq_temp="${fastq##*cutadapt-}";
crplq="${crplq_temp%%-demux*}";
echo "now demuxing ${fastq}";
cutadapt -a small=CTTTCGTACAACCGAGTAGG...CTCCTGAAGTATCTCACGCC -a medium=CGCTACGGCGGTATTGTC...GCTCACCAAGTAAGGTGTAGTAT -a large=TCGATGTTCAACTACTACGC...GCGAGACTCGCTTTGC -O 10 --action=lowercase --revcomp -e 0.15 -o cutadapt-${crplq}-{name}-demux_porechop_land-pads_with_revcomp.fastq ${fastq} > current_cutadapt-${crplq}-segment-demux_landpads_RC_log.txt;
done



#ok going thru the file read counts, noticing some weird stuff. First, it looks like theres overlap between cross 04/08 (which were pooled for PCR2). For example, I used plaq barcodes 85-88 for cross 4 and 93-96 for cross 8. but look at this (only large seg files shown as an examp)! It doesnt seem to happen for cross 3 (plq 81-84) and 6 (plq 89-92), which were pooled but no PCR2.
#but where are the reads for cross 6, plaques 94-96 coming from? contamination?


cross04-plaque83-large,1
cross04-plaque84-large,0
cross04-plaque85-large,29
cross04-plaque86-large,71
cross04-plaque87-large,30
cross04-plaque88-large,45
cross04-plaque89-large,0
cross04-plaque90-large,0
cross04-plaque91-large,2
cross04-plaque92-large,0
cross04-plaque93-large,146
cross04-plaque94-large,135
cross04-plaque95-large,368
cross04-plaque96-large,391

cross08-plaque83-large,3
cross08-plaque84-large,3
cross08-plaque85-large,150
cross08-plaque86-large,316
cross08-plaque87-large,149
cross08-plaque88-large,275
cross08-plaque89-large,4
cross08-plaque90-large,1
cross08-plaque91-large,5
cross08-plaque92-large,0
cross08-plaque93-large,917
cross08-plaque94-large,828
cross08-plaque95-large,2401
cross08-plaque96-large,2334

cross03-plaque79-large,1
cross03-plaque80-large,0
cross03-plaque81-large,616
cross03-plaque82-large,754
cross03-plaque83-large,1913
cross03-plaque84-large,1
cross03-plaque85-large,1
cross03-plaque86-large,0
cross03-plaque87-large,0
cross03-plaque88-large,0
cross03-plaque89-large,1
cross03-plaque90-large,2
cross03-plaque91-large,6
cross03-plaque92-large,0
cross03-plaque93-large,0

cross06-plaque81-large,1
cross06-plaque82-large,2
cross06-plaque83-large,5
cross06-plaque84-large,0
cross06-plaque85-large,2
cross06-plaque86-large,1
cross06-plaque87-large,3
cross06-plaque88-large,2
cross06-plaque89-large,64
cross06-plaque90-large,168
cross06-plaque91-large,187
cross06-plaque92-large,280
cross06-plaque93-large,9
cross06-plaque94-large,8
cross06-plaque95-large,23
cross06-plaque96-large,19

#looking further at cross6, plaques 94, 95, 96:
grep "cross06-plaque94" segment_file_counts.csv
cross06-plaque94,12
cross06-plaque94-large,8
cross06-plaque94-medium,2
cross06-plaque94-small,1
cross06-plaque94-unknown,1

grep "cross06-plaque95" segment_file_counts.csv
cross06-plaque95,31
cross06-plaque95-large,23
cross06-plaque95-medium,8
cross06-plaque95-small,0
cross06-plaque95-unknown,0

grep "cross06-plaque96" segment_file_counts.csv
cross06-plaque96,25
cross06-plaque96-large,19
cross06-plaque96-medium,5
cross06-plaque96-small,0
cross06-plaque96-unknown,1

#Medium is usually the most abundant amplicon.. so maybe this is just a weird error thing?
#after manual inspection of cutadapt output, hits appear real



#some other weird stuff happening too. cross 8 plq 74 was a negative control. Why so many reads?

cross08-plaque73-large,0
cross08-plaque74-large,254
cross08-plaque75-large,5
cross08-plaque76-large,1
#after manual inspection of cutadapt output, hits appear real



#There are virtually 0 small segment reads :( like just in general..
#M seg is the shortest amplicon so I dont *think* its due to filtering out fragments under 200 bp


#whats this?? I didnt use cross 11?
cross11-plaque81-large,0
cross11-plaque82-large,78
cross11-plaque83-large,1
##after manual inspection of cutadapt output, identification of the F barcode (plaque 82 looks legit)
##Ok but most of the CROSS (rev) barcode assignment looks wonky, most reads only have the following section in lowercase:
GTGCctgaccgt
TCGAcataccgt
TCGAcataccgt
CAAGcttacggt

#umm ok i cant figure out where thats coming from, but seems to be happengin due to -O being too small?

#did use cross 8, but not plaque 52?
cross08-plaque51-large,0
cross08-plaque52-large,32
cross08-plaque53-large,1
#manual inspection, also seems to be a weird 



for fastq_file in demux_plaque_S/*.fastq; do
plaque_reads="${fastq_file##*/}"; 
plaque_reads="${plaque_reads#*_}"; 
plaque_reads="${plaque_reads%%.*}"; 




#ok starting over with demultiplexing, 
mkdir second_cutadapt_demultiplexing

#o=14, ~75% of 18 total bp
cutadapt -a file:cross_barcodes.fasta -O 14 --action=lowercase --revcomp -e 0.15 -o {name}_cutadapt-lc_porechop.fastq /group/sldmunozgrp/cysto_LMGSeq08-25/porechop/porechop_land-pads_with_revcomp.fastq > current_cross_cutadapt-lc_porechop_log.txt

#plaques:
#O=18
for fastq in cross*.fastq; 
do cross="${fastq%%_cutadapt*}";
cutadapt -g file:plaque_barcodes.fasta -O 18 --action=lowercase --revcomp -e 0.15 -o ${cross}_{name}_cutadapt-lc_porechop.fastq ${fastq} > current_${cross}_plaque_cutadapt-lc_porechop_log.txt;
done

#um redo becuase I cant find the log files???
#plaques:
#O=18
for fastq in cross??_cutadapt*.fastq; 
do cross="${fastq%%_cutadapt*}";
cutadapt -g file:plaque_barcodes.fasta -O 18 --action=lowercase --revcomp -e 0.15 -o ${cross}_{name}_DELETE.fastq ${fastq} > current_${cross}_plaque_cutadapt-lc_porechop_log.txt;
done
 
#get read counts for each file
for fastq in cross*plaque*.fastq; 
do cross="${fastq%%_cutadapt*}";
count=$( wc -l ${fastq} | awk '{print $1 / 4}'); 
echo "${cross},${count}" >> cross_plaque_read_counts.csv; done


#still some hits that dont belong, but definitely way less:

cross06_plaque88,2
cross06_plaque89,90
cross06_plaque90,133
cross06_plaque91,202
cross06_plaque92,280
cross06_plaque93,12
cross06_plaque94,9
cross06_plaque95,30
cross06_plaque96,24

cross12_plaque54,0
cross12_plaque55,0
cross12_plaque56,15
cross12_plaque57,0
cross12_plaque58,0



#demux on primer, input = cross-plaque files
for fastq in cross??_plaque??_cutadapt-lc_porechop.fastq;  
do crplq_temp="${fastq%_cutadapt-*}"; 
#crplq="${crplq_temp%%-demux*}"; 
echo "now demuxing ${crplq_temp}"; 
cross_temp="${crplq_temp#*cross}"; 
cross="${cross_temp%_plaque*}"; 
echo "cross = ${cross}"; 
plaque="${crplq_temp#*plaque}"; 
echo "plaque = ${plaque}"
cutadapt -a small=CTTTCGTACAACCGAGTAGG...CTCCTGAAGTATCTCACGCC -a medium=CGCTACGGCGGTATTGTC...GCTCACCAAGTAAGGTGTAGTAT -a large=TCGATGTTCAACTACTACGC...GCGAGACTCGCTTTGC -O 10 --action=lowercase --revcomp -e 0.15 -o cross${cross}_plaque${plaque}_{name}_cutadapt-lc_porechop.fastq ${fastq} > cross${cross}_plaque${plaque}_segment_cutadapt-lc_porechop_log.txt;
done


for fastq in cross??_plaque??_cutadapt-lc_porechop.fastq;   
do crplq_temp="${fastq%_cutadapt-*}";  
echo "now demuxing ${crplq_temp}";  
cross_temp="${crplq_temp#*cross}";  
cross="${cross_temp%_plaque*}";  
echo "cross = ${cross}";  
plaque="${crplq_temp#*plaque}";  
echo "plaque = ${plaque}"; mkdir barcode${cross}${plaque}; 
mv cross${cross}_plaque${plaque}_medium*.fastq barcode${cross}${plaque}/barcode${cross}${plaque}_M_reads.fastq;
mv cross${cross}_plaque${plaque}_small*.fastq barcode${cross}${plaque}/barcode${cross}${plaque}_S_reads.fastq; 
mv cross${cross}_plaque${plaque}_large*.fastq barcode${cross}${plaque}/barcode${cross}${plaque}_L_reads.fastq; done


#trim lower case text from fastqs
#awk to remove lowercase 

for i in barcode*/*.fastq; do filetmp="${i%.fastq*}";  echo "${filetmp}"; awk '{
  if (NR % 4 == 2) {
    # Sequence line
    seq = $0
    gsub(/[a-z]/, "", seq)
    print seq
  } else if (NR % 4 == 0) {
    # Quality line
    # Match quality scores to filtered sequence
    n = length(seq)
    print substr($0, 1, n)
  } else {
    # Header or plus line
    print
  }
}' ${filetmp}.fastq > ${filetmp}_uc.fastq; done


#ok shoullldddd be in the right format now??






#ok jsut to test if cutaapt is workign and compare to the old data, lets move to the fortmat that works with the existing script

for fastq in *plaque*-demux*.fastq; 
do crplq_temp="${fastq##*cutadapt-}";
crplq="${crplq_temp%%-demux*}";
echo "now demuxing ${crplq}";



























ONT duplex bascalling attempt:

~/dorado/bin/dorado duplex -r --device cuda:all --threads 20 --emit-fastq -o duplex_sup_basecall sup pod5
#jus kidding it gives a warning that fastq output not recommended so ill take that off and allow the default. its unclear which is recommended

#lost connectivity, rerun inn a tmux sesh:

~/dorado/bin/dorado duplex -r --device cuda:all --threads 40 -o rerun_duplex_sup_basecall sup pod5

[2025-09-14 10:16:02.602] [info] Running: "duplex" "-r" "--device" "cuda:all" "--threads" "40" "-o" "rerun_duplex_sup_basecall" "sup" "pod5"
[2025-09-14 10:16:02.627] [info] > No duplex pairs file provided, pairing will be performed automatically
[2025-09-14 10:16:02.750] [info]  - downloading dna_r10.4.1_e8.2_400bps_sup@v5.2.0 with httplib
[2025-09-14 10:16:20.779] [info]  - downloading dna_r10.4.1_e8.2_5khz_stereo@v1.4 with httplib
[2025-09-14 10:16:22.401] [info] Using CUDA devices:
[2025-09-14 10:16:22.401] [info] cuda:0 - NVIDIA GeForce RTX 3080
[2025-09-14 10:16:23.095] [info] Calculating optimized batch size for GPU "NVIDIA GeForce RTX 3080" and model dna_r10.4.1_e8.2_400bps_sup@v5.2.0. Full benchm
arking will run for this device, which may take some time.
[2025-09-14 10:16:23.461] [info] cuda:0 using chunk size 12288, batch size 64
[2025-09-14 10:16:23.705] [info] Calculating optimized batch size for GPU "NVIDIA GeForce RTX 3080" and model dna_r10.4.1_e8.2_5khz_stereo@v1.4. Full benchma
rking will run for this device, which may take some time.
[2025-09-14 10:16:23.952] [info] cuda:0 using chunk size 10000, batch size 320
[2025-09-14 10:16:24.129] [info] > Starting Stereo Duplex pipeline
[2025-09-14 10:16:24.135] [info] > Reading read channel info
[2025-09-14 10:16:44.175] [info] > Processed read channel info
[2025-09-15 07:15:53.809] [info] > Finished in (ms): 75569670
[2025-09-15 07:15:53.809] [info] > Simplex reads basecalled: 20226344
[2025-09-15 07:15:53.809] [info] > Simplex reads filtered: 901
[2025-09-15 07:15:53.809] [info] > Duplex reads basecalled: 1697341
[2025-09-15 07:15:53.809] [info] > Duplex rate: 10.896313%
[2025-09-15 07:15:53.810] [info] > Basecalled @ Bases/s: 2.564030e+05

#avg duplex read length:
awk 'NR%4==2 {sum+=length($0); count++} END {if (count > 0) print "Average read length:", sum/count; else print "No reads found."}' FBC73506_bam_pass_d5fa85e0__0_DUPLEX.fastq
=Average read length: 589.81

#avg simplex reads length
awk 'NR%4==2 {sum+=length($0); count++} END {if (count > 0) print "Average read length:", sum/count; else print "No reads found."}' FBC73506_bam_pass_d5fa85e0_b727e37a_0_SIMPLEX.fastq
Average read length: 869.548






#also rerunning simplex but with read splitting, for comparison to post-basecalling reads splitting (above)

~/dorado/bin/dorado basecaller -v -r --emit-fastq --no-trim --device cuda:all -o sup_basecall_split sup pod5


[2025-09-15 12:46:02.424] [info] Running: "basecaller" "-v" "-r" "--emit-fastq" "--no-trim" "--device" "cuda:all" "-o" "sup_basecall_split" "sup" "pod5"
[2025-09-15 12:46:02.478] [info]  - downloading dna_r10.4.1_e8.2_400bps_sup@v5.2.0 with httplib
[2025-09-15 12:46:20.791] [info] > Creating basecall pipeline
[2025-09-15 12:46:20.874] [info] Using CUDA devices:
[2025-09-15 12:46:20.874] [info] cuda:0 - NVIDIA GeForce RTX 3080
[2025-09-15 12:46:20.886] [debug] TxEncoderStack: use_koi_tiled true.
[2025-09-15 12:46:20.886] [debug] TxEncoderStack: use_koi_volta_tiled false.
[2025-09-15 12:46:21.458] [debug] cuda:0 memory available: 9.58GB
[2025-09-15 12:46:21.458] [debug] cuda:0 memory limit 8.58GB
[2025-09-15 12:46:21.458] [debug] cuda:0 maximum safe estimated batch size at chunk size 12288 is 96
[2025-09-15 12:46:21.458] [debug] cuda:0 maximum safe estimated batch size at chunk size 6144 is 192
[2025-09-15 12:46:21.458] [debug] Auto batchsize cuda:0: testing up to 192 in steps of 32
[2025-09-15 12:46:21.459] [info] Calculating optimized batch size for GPU "NVIDIA GeForce RTX 3080" and model dna_r10.4.1_e8.2_400bps_sup@v5.2.0. Full benchmarking will run for this device, which may take some time.
[2025-09-15 12:46:21.666] [debug] Auto batchsize cuda:0: 32, time per chunk 0.449024 ms
[2025-09-15 12:46:21.748] [debug] Auto batchsize cuda:0: 64, time per chunk 0.366592 ms
[2025-09-15 12:46:21.828] [debug] Auto batchsize cuda:0: 96, time per chunk 0.317556 ms
[2025-09-15 12:46:21.925] [debug] Auto batchsize cuda:0: 128, time per chunk 0.310182 ms
[2025-09-15 12:46:22.042] [debug] Auto batchsize cuda:0: 160, time per chunk 0.300205 ms
[2025-09-15 12:46:22.176] [debug] Auto batchsize cuda:0: 192, time per chunk 0.295632 ms
[2025-09-15 12:46:22.176] [debug] Adding chunk timings to internal cache for GPU NVIDIA GeForce RTX 3080, model dna_r10.4.1_e8.2_400bps_sup@v5.2.0 (6 entries)
[2025-09-15 12:46:22.176] [debug] Largest batch size for cuda:0: 192, time per chunk 0.295632 ms
[2025-09-15 12:46:22.176] [debug] Final batch size for cuda:0[0]: 96
[2025-09-15 12:46:22.176] [debug] Final batch size for cuda:0[1]: 192
[2025-09-15 12:46:22.177] [info] cuda:0 using chunk size 12288, batch size 96
[2025-09-15 12:46:22.177] [debug] cuda:0 Model memory 6.85GB
[2025-09-15 12:46:22.177] [debug] cuda:0 Decode memory 0.83GB
[2025-09-15 12:46:22.383] [info] cuda:0 using chunk size 6144, batch size 192
[2025-09-15 12:46:22.383] [debug] cuda:0 Model memory 6.85GB
[2025-09-15 12:46:22.383] [debug] cuda:0 Decode memory 0.83GB
[2025-09-15 12:46:22.645] [info]  - Note: FASTQ output is not recommended as not all data can be preserved.
[2025-09-15 12:46:22.645] [info]  - Note: FASTQ output is not recommended as not all data can be preserved.
[2025-09-15 12:46:22.645] [debug] BasecallerNode chunk size 12288
[2025-09-15 12:46:22.645] [debug] BasecallerNode chunk size 6144
[2025-09-15 12:46:22.647] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_65.pod5
[2025-09-15 12:50:12.989] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_27.pod5
[2025-09-15 13:10:28.153] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_20.pod5
[2025-09-15 13:32:02.444] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_41.pod5
[2025-09-15 13:49:12.765] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_68.pod5
[2025-09-15 13:49:36.659] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_54.pod5
[2025-09-15 13:57:39.074] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_6.pod5
[2025-09-15 14:25:29.691] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_55.pod5
[2025-09-15 14:32:16.264] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_51.pod5
[2025-09-15 14:42:44.385] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_19.pod5
[2025-09-15 15:05:39.276] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_11.pod5
[2025-09-15 15:32:49.396] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_0.pod5
[2025-09-15 16:05:51.869] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_18.pod5
[2025-09-15 16:27:22.996] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_23.pod5
[2025-09-15 16:47:01.224] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_1.pod5
[2025-09-15 17:19:47.202] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_38.pod5
[2025-09-15 17:35:50.804] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_22.pod5
[2025-09-15 17:58:27.118] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_53.pod5
[2025-09-15 18:04:38.046] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_64.pod5
[2025-09-15 18:06:02.725] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_56.pod5
[2025-09-15 18:09:23.021] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_45.pod5
[2025-09-15 18:23:33.202] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_12.pod5
[2025-09-15 18:48:28.002] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_7.pod5
[2025-09-15 19:16:07.453] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_5.pod5
[2025-09-15 19:44:35.586] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_25.pod5
[2025-09-15 20:05:41.061] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_52.pod5
[2025-09-15 20:18:09.019] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_40.pod5
[2025-09-15 20:32:46.616] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_33.pod5
[2025-09-15 20:51:33.934] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_9.pod5
[2025-09-15 21:17:51.589] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_58.pod5
[2025-09-15 21:20:09.333] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_50.pod5
[2025-09-15 21:31:28.723] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_29.pod5
[2025-09-15 21:51:01.527] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_15.pod5
[2025-09-15 22:14:27.048] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_24.pod5
[2025-09-15 22:36:01.636] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_4.pod5
[2025-09-15 23:06:39.111] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_66.pod5
[2025-09-15 23:07:43.929] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_46.pod5
[2025-09-15 23:20:38.160] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_60.pod5
[2025-09-15 23:25:43.819] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_36.pod5
[2025-09-15 23:44:01.429] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_30.pod5
[2025-09-16 00:05:11.319] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_3.pod5
[2025-09-16 00:37:16.023] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_35.pod5
[2025-09-16 00:54:05.770] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_42.pod5
[2025-09-16 01:10:58.993] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_59.pod5
[2025-09-16 01:13:30.506] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_13.pod5
[2025-09-16 01:38:26.163] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_47.pod5
[2025-09-16 01:51:56.560] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_32.pod5
[2025-09-16 02:08:22.711] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_39.pod5
[2025-09-16 02:25:05.363] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_14.pod5
[2025-09-16 02:48:58.977] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_37.pod5
[2025-09-16 03:05:33.518] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_43.pod5
[2025-09-16 03:20:12.453] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_63.pod5
[2025-09-16 03:22:07.317] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_44.pod5
[2025-09-16 03:35:52.076] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_10.pod5
[2025-09-16 04:04:38.616] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_8.pod5
[2025-09-16 04:32:19.596] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_61.pod5
[2025-09-16 04:34:32.951] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_62.pod5
[2025-09-16 04:36:37.894] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_26.pod5
[2025-09-16 04:56:35.166] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_34.pod5
[2025-09-16 05:13:14.723] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_49.pod5
[2025-09-16 05:25:26.744] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_48.pod5
[2025-09-16 05:38:22.027] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_28.pod5
[2025-09-16 05:57:45.061] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_17.pod5
[2025-09-16 06:19:23.713] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_21.pod5
[2025-09-16 06:40:52.967] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_31.pod5
[2025-09-16 06:56:34.325] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_67.pod5
[2025-09-16 06:58:02.005] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_57.pod5
[2025-09-16 07:00:34.477] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_16.pod5
[2025-09-16 07:24:52.167] [debug] Load reads from file pod5/FBC73506_d5fa85e0_b727e37a_2.pod5
[2025-09-16 07:57:39.077] [info] > Finished in (ms): 69076429
[2025-09-16 07:57:39.077] [info] > Simplex reads basecalled: 20226344
[2025-09-16 07:57:39.077] [info] > Simplex reads filtered: 901
[2025-09-16 07:57:39.077] [info] > Basecalled @ Samples/s: 3.532851e+06
[2025-09-16 07:57:39.077] [debug] > Including Padding @ Samples/s: 5.072e+06 (69.65%)
[2025-09-16 07:57:39.081] [debug] Deleting temporary model path: /home/user/Cysto_LMGSeq_barcode_test_


#avg read length with splitting turned on:
awk 'NR%4==2 {sum+=length($0); count++} END {if (count > 0) print "Average read length:", sum/count; else print "No reads found."}' FBC73506_fastq_pass_d5fa85e0_b727e37a_0_split.fastq
Average read length: 877.997

#hmm.. kinda long. 
#how many reads?
20805897
#OK ngl i was expecting it to be a lot more with splotting enabled



